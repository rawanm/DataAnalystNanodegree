{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with MongoDB\n",
    "\n",
    "## Open Street Map: Pittsburgh, Pennsylvania, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Project will go through the process of assessing and cleaning OpenStreetMap data for the Pittsburgh, PA area: https://mapzen.com/data/metro-extracts/metro/pittsburgh_pennsylvania/ </br> \n",
    "After exploring and performing audting on sample of the area, I have discovered problems that compromises the quality of the data. \n",
    "\n",
    "I will show the process of audting, followed by problems encountered, then my attemps at cleaning and correcting some of these problems, finally will write the data to JSON file and import it to MongoDB and run some queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring and Auditing the Data:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large number of tags and variables in this dataset, I will set my focus of the address data of nodes. \n",
    "\n",
    "### Problems: \n",
    "\n",
    "1. One of the main concerns is the street names having different forms and abbreviation, for example: \n",
    "<br>\n",
    "<b>Airport Boulevard \n",
    "<br> \n",
    "Airport Blvd</b>\n",
    "\n",
    "2. Address Zip code, this field generated many problems regarding the format, for example some have xxxxx-xxxx, the extended four digits are used for postal delivery services. Also, some have leading character representing the state, this will also be removed.\n",
    "\n",
    "<b> Streets: </b>\n",
    "First, after running and exploring the auditing of street types multible times, I have constructed list of all expected street types,  I also have a list of the correct street types, meaning they don't require any cleaning. For the ones that don't fall into the correct street types, there will be method to correct it based on special mapping. \n",
    "<br> \n",
    "One on the problems with street names here is that they may have suffix so this must be taken into considiration before implying the street type is incorrect. \n",
    "\n",
    "Running this method: audit_street_names(filename): \n",
    "<br>\n",
    "It will give and overview understanding of the process on sample data and some statistic info of the addr:street variable. \n",
    "\n",
    "<b> ZIP-Codes: </b> \n",
    "In order to clean this field I will remove the extended four digits part, in addition to all leading characters.\n",
    "<br>\n",
    "Running this method: audit_postal_codes(filename): \n",
    "<br>\n",
    "For example: 15213-1503 => 15213\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages: \n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict  # available in Python 2.5 and newer\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_FILE = \"pittsburgh_pennsylvania.osm\"\n",
    "SAMPLE_FILE = \"sample_sample_pittsburgh_pennsylvania.osm\"\n",
    "DB_NAME = \"pittsburgh_pennsylvania\"\n",
    "JSON_FILE = \"pittsburgh_pennsylvania.osm.json\"\n",
    "COLLECTION_NAME = \"pittsburgh_pennsylvania_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to update street names that need correction based on supplied mappaing: \n",
    "def update_street_name (mapping, street_name, suffix):\n",
    "    name = street_name\n",
    "    street_type = street_type_re.search(street_name).group() \n",
    "    if  street_type not in correct_street_types:\n",
    "        street_data['incorrect_streets'] += 1\n",
    "        name = street_name.replace(street_type, mapping[street_type])\n",
    "        street_data['corrected_streets'] += 1\n",
    "    if suffix in street_suffix:\n",
    "        name = name + \" - \" + suffix\n",
    "        street_data['corrected_streets_suffix'] += 1\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_code_re = re.compile(r'\\d+', re.IGNORECASE) \n",
    "\n",
    "def update_postal_code (postcode):\n",
    "    m = zip_code_re.search(postcode)\n",
    "    if m:\n",
    "        match = m.group(0)\n",
    "        return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Writing to JSON file:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data we're interetsed in is ready for processing, this phase will iterate over OpenStreetMap data and convert it after cleaning to JSON file, without modifying the original data file. I have chose JSON becuase in the next phase it will be imported to MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = write_json(OSM_FILE, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Proccessing data with MongoDB\n",
    "\n",
    "After exporting the cleaned data to JSON file, it's imported to MongoDB using this command: \n",
    "\n",
    "``` \n",
    "mongoimport -d pittsburgh_pennsylvania -c pittsburgh_pennsylvania_data --file pittsburgh_pennsylvania.osm.json \n",
    "```\n",
    "\n",
    "Now will do some data proccessing and querying to provide an overview statistics about the data, and other ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to get database by its name: \n",
    "def get_db(db_name):\n",
    "    # For local use\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Name: pittsburgh_pennsylvania\n",
      "Sample Data Point: \n",
      "{u'_id': ObjectId('58f9354e6aa9fc9fc8e23f32'),\n",
      " u'address': {},\n",
      " u'created': {u'changeset': u'134337',\n",
      "              u'timestamp': u'2007-07-09T03:10:11Z',\n",
      "              u'uid': u'867',\n",
      "              u'user': u'tscofield',\n",
      "              u'version': u'1'},\n",
      " u'id': u'31479671',\n",
      " u'pos': [u'-80.10257', u'40.1688251'],\n",
      " u'type': u'node',\n",
      " u'visible': u'true'}\n",
      "Data Statisisc: \n",
      "{'JSON file size - MB': 726.0528079999999,\n",
      " 'OSM file size - MB': 476.13142,\n",
      " 'Total documents count': 2370018,\n",
      " 'Total nodes count': 2146449,\n",
      " 'Total unique users': 1424,\n",
      " 'Total ways count': 223569}\n"
     ]
    }
   ],
   "source": [
    "# info about the data: \n",
    "if __name__ == \"__main__\":\n",
    "    db = get_db(DB_NAME)\n",
    "    print \"Database Name: \" + db.name\n",
    "    \n",
    "    print \"Sample Data Point: \"\n",
    "    pprint.pprint (db.pittsburgh_pennsylvania_data.find_one())\n",
    "    \n",
    "    print \"Data Statisisc: \"\n",
    "    pprint.pprint(get_overview_statistics(OSM_FILE, JSON_FILE, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_overview_statistics(osm_filename, json_filename, db):\n",
    "    statistics = {}\n",
    "    statistics['OSM file size - MB'] = os.path.getsize(osm_filename) * (1e-6)\n",
    "    statistics['JSON file size - MB'] = os.path.getsize(json_filename) * (1e-6)\n",
    "    statistics['Total documents count'] = db.pittsburgh_pennsylvania_data.find().count()\n",
    "    statistics['Total nodes count'] = db.pittsburgh_pennsylvania_data.find({\"type\": \"node\"}).count()\n",
    "    statistics['Total ways count'] = db.pittsburgh_pennsylvania_data.find({\"type\": \"way\"}).count()\n",
    "    statistics['Total unique users'] = len(db.pittsburgh_pennsylvania_data.distinct(\"created.uid\"))\n",
    "    \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def query_db (db):    \n",
    "    print \"Top Contributing User: \"\n",
    "    top_users = db.pittsburgh_pennsylvania_data.aggregate([{\"$group\":{\"_id\":\"$created.uid\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":1}])\n",
    "    for user in top_users:\n",
    "        print user\n",
    "\n",
    "    print \"Top Type of Amenities: \"\n",
    "    top_amenities = db.pittsburgh_pennsylvania_data.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":1}])\n",
    "    for amenity in top_amenities:\n",
    "        pprint.pprint(amenity)\n",
    "        \n",
    "    print \"Top Type of Shops: \"     \n",
    "    top_shops = db.pittsburgh_pennsylvania_data.aggregate([{\"$match\":{\"shop\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$shop\",\"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":1}])\n",
    "    for shop in top_shops:\n",
    "        pprint.pprint(shop)\n",
    "     \n",
    "    print \"Top Type of cuisines: \"\n",
    "    top_cuisines = db.pittsburgh_pennsylvania_data.aggregate([{\"$match\":{\"cuisine\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$cuisine\",\"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":1}])\n",
    "    for cuisine in top_cuisines:\n",
    "        pprint.pprint(cuisine)\n",
    "    \n",
    "    print \"Most Popular Shops: \"\n",
    "    shop_results = db.pittsburgh_pennsylvania_data.aggregate([{\"$match\":{\"shop\":{\"$exists\":1}, \"name\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$name\",\"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}},{\"$limit\":3}])\n",
    "    for result in shop_results:\n",
    "        pprint.pprint(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Contributing User: \n",
      "{u'count': 307248, u'_id': u'2835510'}\n",
      "Top Type of Amenities: \n",
      "{u'_id': u'parking', u'count': 4440}\n",
      "Top Type of Shops: \n",
      "{u'_id': u'car_repair', u'count': 284}\n",
      "Top Type of cuisines: \n",
      "{u'_id': u'american', u'count': 174}\n",
      "Most Popular Shops: \n",
      "{u'_id': u'Giant Eagle', u'count': 54}\n",
      "{u'_id': u'Dollar General', u'count': 26}\n",
      "{u'_id': u'Sheetz', u'count': 20}\n"
     ]
    }
   ],
   "source": [
    "# get additional info about db: \n",
    "if __name__ == \"__main__\":\n",
    "    db = get_db(DB_NAME)\n",
    "    query_db(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "### 3. Other Ideas About  the dataset: \n",
    "\n",
    "One idea that concerns me is how schools serves districts, and how we can improve and utilize their services, for example I will attempt to get the number of houses whithin 10 miles: I will get any school using mongoDB query, finding the surrounding premises will require some advanced geometry calculation so for the sake of this report I will look for houses on the same street as the school, to give an overview of the idea. Of course this is not very accurate because not all of these points would be households but given the metadata we have we can’t distinguish between houses or other types of locations. To further improve such analysis we need more data about the school and district households.  \n",
    "<br>\n",
    "I would like to see integrated data about public transportations, for example bus routes, and some detailed data about rides, hours, dates, whether. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education Stats: \n",
      "{u'_id': u'school', u'count': 1401}\n",
      "{u'_id': u'university', u'count': 66}\n",
      "{u'_id': u'college', u'count': 14}\n",
      "{u'_id': u'childcare', u'count': 3}\n",
      "{u'_id': u'education_centre', u'count': 1}\n",
      "{u'_id': u'grade_school', u'count': 1}\n",
      "Sample School: \n",
      "{u'_id': ObjectId('58faf8feaf2d3979fe905765'),\n",
      " u'address': {u'city': u'Penn Hills',\n",
      "              u'housenumber': u'3123',\n",
      "              u'postcode': u'15147',\n",
      "              u'state': u'PA',\n",
      "              u'street': u'Long Hollow Road'},\n",
      " u'amenity': u'school',\n",
      " u'created': {u'changeset': u'19066334',\n",
      "              u'timestamp': u'2013-11-23T03:40:50Z',\n",
      "              u'uid': u'1813632',\n",
      "              u'user': u'JeanLiu',\n",
      "              u'version': u'5'},\n",
      " u'id': u'158554556',\n",
      " u'name': u'Sunset Valley',\n",
      " u'pos': [u'-79.7434092', u'40.3055684'],\n",
      " u'type': u'node',\n",
      " u'visible': u'true'}\n",
      "Nearby houses count: \n",
      "7250\n"
     ]
    }
   ],
   "source": [
    "    print \"Education Stats: \"\n",
    "    edu_results = db.pittsburgh_pennsylvania_data.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\": {'$in': [\"university\",\"school\", \"college\", \"grade_school\", \"childcare\", \"education_centre\"]}}}, {\"$group\":{\"_id\":\"$amenity\",\"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])\n",
    "    for result in edu_results:\n",
    "        pprint.pprint(result)\n",
    "        \n",
    "        \n",
    "    school_sample = db.pittsburgh_pennsylvania_data.find_one({\"amenity\": \"school\"})\n",
    "    print \"Sample School: \"\n",
    "    pprint.pprint (school_sample)\n",
    "    nearby_houses = db.pittsburgh_pennsylvania_data.find ({\"address.street\": school_sample[\"address\"][\"street\"]}, {\"type\": \"node\"}).count()\n",
    "    print \"Nearby houses count: \"\n",
    "    print nearby_houses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion and Imporvements: \n",
    "\n",
    "#### Imporvements: \n",
    "Now that we have part of open street map data cleaned, it is important to mention some improvements that could be done to further enhance the quality of data. One major component it the source of data, some nodes have data imported from TIGER (http://wiki.openstreetmap.org/wiki/TIGER), while doing some research it's advised that it may have incorrect data points and requires fix-up. Nonetheless, it can be seen that some data only apparent in TIGER tags, we can still use these but more cleaning and validating the accuracy and consistency is highly required. \n",
    "<br>\n",
    "I have had several problems with the data up to this point after wrangling and cleaning. The accuracy and correctness is debatable! The address and position data when doing quick google map search are not consistent, there are many duplications as well but it requires more digging and familiarity with the area to detect. \n",
    "<br>\n",
    "<br>\n",
    "I think there could be some suggestion to improve the data collection process, for example add some soft of rating of the data, where users can rate each others entries. \n",
    "<br>\n",
    "There could be some implemented analysis to establish baseline for the metadata required for each type of nodes, for example if the entry is restaurant the user would be required to enter more details; cuisine, hours, phone number. \n",
    "<br>\n",
    "This analysis could also beneficial to detect main details of certain location based on nearby data points, for example: state name, country, etc, and provide it for the user to ensure accuracy. Also for big areas it would be useful to ask users to provide as specific details as possible, for example: district name, neighborhood, etc.   \n",
    "#### Anticipated Problems:\n",
    "When dealing with data there’s always chance of errors, so we can't depend on suggested analysis to ensure the availability of further details, this could also cause limitation of data collection. \n",
    "Also, when asking users to rate entries as suggested to improve quality, there could be problems in getting users to do so, we need to implement mechanisms or reward system to motivate users. \n",
    "<br>\n",
    "#### Conclusion:\n",
    "In conclusion the improvements are countless but nonetheless the data collected is huge and with more wrangling and cleaning we can establish much more insights. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
